{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf95725",
   "metadata": {},
   "source": [
    "# **Iterative Methods for Sparse Linear Systems (From Scratch)**\n",
    "\n",
    "**Author:** Kaj Hansteen Izora  \n",
    "\n",
    "This project showcases several iterative methods for solving large, sparse linear systems of the form $A x = b$. All core routines—steepest descent, conjugate gradient, and preconditioned conjugate gradient—are implemented from the ground up, giving a clear illustration of how these algorithms work under the hood. Additionally, the code demonstrates how to handle sparse matrices efficiently using CSR (Compressed Sparse Row) format, along with how to incorporate different preconditioners (Jacobi and symmetric Gauss-Seidel).\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Overview of Key Algorithms**\n",
    "\n",
    "1. **Steepest Descent**  \n",
    "   - Two variants (following Algorithm 1 and Algorithm 2 from standard references/notes).  \n",
    "   - Uses the negative gradient of the residual to iteratively improve the solution.  \n",
    "   - Simple to implement but can be relatively slow to converge.\n",
    "\n",
    "2. **Conjugate Gradient (CG)**  \n",
    "   - A classic method for positive-definite systems that significantly outperforms plain steepest descent.  \n",
    "   - Iterates in directions that are conjugate with respect to the matrix $A$, speeding up convergence.\n",
    "\n",
    "3. **Preconditioned Conjugate Gradient (PCG)**  \n",
    "   - Applies a preconditioner $M$ to reduce the condition number of $A$.  \n",
    "   - We use both **Jacobi** and **symmetric Gauss-Seidel** preconditioners.  \n",
    "   - Typically converges faster than standard CG, albeit with a more expensive iteration step.\n",
    "\n",
    "We also explore theoretical convergence estimates (related to the condition number) and compare them with observed iteration counts.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Working with Sparse CSR Matrices**\n",
    "\n",
    "### **2.1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9468675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b1561",
   "metadata": {},
   "source": [
    "### **2.2. Loading and CSR Helper Functions**\n",
    "\n",
    "Below are utility functions that load a matrix from a file into CSR format, extract the number of rows, and perform a matrix-vector multiplication—all written from scratch (rather than using built-in methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08c26f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_as_csr(filename):\n",
    "    \"\"\"\n",
    "    Loads a sparse matrix from a custom file format into CSR representation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the file containing the matrix data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    (I, J, V) : tuple of np.ndarray\n",
    "        CSR representation: I (index pointer), J (column indices), and V (nonzero values).\n",
    "    \"\"\"\n",
    "    # Read matrix dimensions and number of nonzeros\n",
    "    m, n, nnz = np.genfromtxt(filename, max_rows=1, dtype=None)\n",
    "    # Read the data as a list of tuples [(row_i, col_j, value), ...]\n",
    "    data = np.genfromtxt(filename, skip_header=1, dtype=None)\n",
    "\n",
    "    # Allocate arrays\n",
    "    I = np.zeros(m + 1, dtype=int)\n",
    "    J = np.zeros(nnz, dtype=int)\n",
    "    V = np.zeros(nnz)\n",
    "\n",
    "    # Count nonzeros per row\n",
    "    for k in range(nnz):\n",
    "        i = data[k][0]\n",
    "        I[i+1] += 1\n",
    "\n",
    "    # Convert to cumulative sums\n",
    "    for i in range(int(m)):\n",
    "        I[i+1] += I[i]\n",
    "\n",
    "    # Fill J and V based on row placements\n",
    "    for k in range(nnz):\n",
    "        i, j, val = data[k]\n",
    "        idx = I[i]\n",
    "        I[i] += 1\n",
    "        J[idx] = j\n",
    "        V[idx] = val\n",
    "\n",
    "    # Shift I array back to correct pointers\n",
    "    for i in reversed(range(int(m))):\n",
    "        I[i+1] = I[i]\n",
    "    I[0] = 0\n",
    "\n",
    "    return (I, J, V)\n",
    "\n",
    "def num_rows(A):\n",
    "    \"\"\"\n",
    "    Returns the number of rows in the CSR matrix A = (I, J, V).\n",
    "    \"\"\"\n",
    "    return len(A[0]) - 1\n",
    "\n",
    "def matvec(A, v):\n",
    "    \"\"\"\n",
    "    Performs the matrix-vector product for a CSR matrix A and vector v.\n",
    "    \"\"\"\n",
    "    I, J, V = A\n",
    "    m = num_rows(A)\n",
    "    w = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        begin = I[i]\n",
    "        end = I[i+1]\n",
    "        for idx in range(begin, end):\n",
    "            j = J[idx]\n",
    "            w[i] += V[idx] * v[j]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ea723",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3. Steepest Descent**\n",
    "\n",
    "Steepest descent is often taught as the most direct gradient-based approach to solving $Ax = b$. We show two implementations:\n",
    "\n",
    "- **`steepest_descent_1`** follows a classical textbook/lecture style of computing the residual and step size each iteration.\n",
    "- **`steepest_descent_2`** stores one less matrix-vector product per iteration.\n",
    "\n",
    "### **3.1. Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6f0f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_1(A, b, tol, max_it=2000):\n",
    "    \"\"\"\n",
    "    Implements classical steepest descent (Algorithm 1 style).\n",
    "    Converges when the norm of the residual is below `tol`.\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    x = np.zeros(n)\n",
    "    it = 0\n",
    "    while True:\n",
    "        r = b - matvec(A, x)\n",
    "        alpha = np.dot(r, r) / np.dot(r, matvec(A, r))\n",
    "        x = x + alpha * r\n",
    "        it += 1\n",
    "        \n",
    "        if np.linalg.norm(r) < tol:\n",
    "            print(f\"Converged in {it} iterations\")\n",
    "            break\n",
    "        if it >= max_it:\n",
    "            print(\"Max iterations Reached\")\n",
    "            break\n",
    "    return x, it\n",
    "\n",
    "def steepest_descent_2(A, b, tol, max_it=2000):\n",
    "    \"\"\"\n",
    "    Steepest descent with a slight optimization (Algorithm 2 style).\n",
    "    Stores `z = A r` to avoid re-computing it in the alpha step.\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    x = np.zeros(n)\n",
    "    r = b - matvec(A, x)\n",
    "    z = np.zeros(n)\n",
    "    alpha = 0\n",
    "    it = 0\n",
    "    while True:\n",
    "        r = r - alpha * z\n",
    "        z = matvec(A, r)\n",
    "        alpha = np.dot(r, r) / np.dot(r, z)\n",
    "        x = x + alpha * r\n",
    "        it += 1\n",
    "        \n",
    "        if np.linalg.norm(r) < tol:\n",
    "            print(f\"Converged in {it} iterations\")\n",
    "            break\n",
    "        if it >= max_it:\n",
    "            print(\"Max iterations Reached\")\n",
    "            break\n",
    "    return x, it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06044fb6",
   "metadata": {},
   "source": [
    "### **3.2. Testing Steepest Descent**\n",
    "\n",
    "We load four example matrices (`16.txt`, `25.txt`, `50.txt`, `64.txt`) which vary in size and condition numbers. Each matrix is solved against a randomly generated right-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3356e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 607 iterations\n",
      "Converged in 607 iterations\n",
      "\n",
      "File: 16.txt\n",
      "Steepest Descent 1 iterations: 607\n",
      "Steepest Descent 2 iterations: 607\n",
      "\n",
      "actual answer [0.53198622 0.09899997 0.68102428 0.00172617 0.55390107 0.87347616\n",
      " 0.46273783 0.9043583  0.83372745 0.19049786 0.66853579 0.16824415\n",
      " 0.22832795 0.53371259 0.25781822 0.4518783 ]\n",
      "SD1 is correct\n",
      "SD2 is correct\n",
      "------------------------------------------------------------\n",
      "Converged in 837 iterations\n",
      "Converged in 837 iterations\n",
      "\n",
      "File: 25.txt\n",
      "Steepest Descent 1 iterations: 837\n",
      "Steepest Descent 2 iterations: 837\n",
      "\n",
      "actual answer [ 5.16246348e+01 -5.72735133e+00 -1.41688466e+01  3.36685402e+00\n",
      "  1.30856991e+01  2.35415246e+00  3.95129133e+00  3.11327406e+00\n",
      " -6.63028231e+00  5.25829866e+00  5.40795979e+00 -7.92076305e-01\n",
      "  2.09922311e+01 -5.72575914e-02  2.15755518e+01 -9.94338413e-01\n",
      "  5.61752076e+00  7.90741857e+00  5.37835719e+00  1.22606610e+01\n",
      " -8.65993019e+00  4.56114180e+00  3.94504901e+01  1.36704408e+00\n",
      "  6.15144214e+01]\n",
      "SD1 is correct\n",
      "SD2 is correct\n",
      "------------------------------------------------------------\n",
      "Converged in 771 iterations\n",
      "Converged in 771 iterations\n",
      "\n",
      "File: 50.txt\n",
      "Steepest Descent 1 iterations: 771\n",
      "Steepest Descent 2 iterations: 771\n",
      "\n",
      "actual answer [-2.8080864   9.23668557 28.29912267 18.52262027 42.58999449 13.10938951\n",
      "  1.06616882 10.31348592  1.15300191 39.65582596  6.4944548  -7.7851771\n",
      " 27.26371422  3.72719999  7.73662715  2.20920557 30.74917317  1.59381293\n",
      "  2.16730074  1.42987395  3.67999303  0.86518831 19.05932823  5.62091452\n",
      "  4.84560238  5.56481643 10.65794813  9.37700866  7.6151763   9.92872726\n",
      " -4.27698785  3.16497636 -4.8450979  16.03866042  9.24558656 -7.22707956\n",
      " 17.36787033 15.44359543 23.04400202 11.59657089 35.5780893   8.11006317\n",
      " 15.58855842  9.66304872 24.47834898  6.0020302  17.13477001 57.31273463\n",
      " 32.14493981 16.96234526]\n",
      "SD1 is correct\n",
      "SD2 is correct\n",
      "------------------------------------------------------------\n",
      "Converged in 1299 iterations\n",
      "Converged in 1299 iterations\n",
      "\n",
      "File: 64.txt\n",
      "Steepest Descent 1 iterations: 1299\n",
      "Steepest Descent 2 iterations: 1299\n",
      "\n",
      "actual answer [1.35658774 0.87592103 1.38843097 1.56763172 1.23897498 1.32127788\n",
      " 1.53041517 1.18781373 0.93622071 1.54491411 1.93620237 1.93697138\n",
      " 1.73008258 1.12639409 0.58984019 1.18092964 1.06518523 0.31167793\n",
      " 1.81930855 1.79526255 1.15755785 0.55008498 0.71944156 1.97357627\n",
      " 1.981604   1.61365999 0.34239027 1.9857609  0.02124927 1.02929036\n",
      " 1.52817103 1.07549507 0.46044719 1.0802518  0.56274028 0.51914785\n",
      " 0.31244006 1.14291915 0.78360505 0.64333702 1.65024263 1.41724975\n",
      " 1.74614042 0.26130324 2.03112921 0.87216845 1.30887995 1.0019237\n",
      " 0.72656772 1.05408224 2.09811391 0.88644213 0.39208277 1.2950662\n",
      " 1.77943248 1.25648341 0.44248737 0.28113987 0.97774685 0.43267922\n",
      " 1.34470986 0.84392556 0.41618749 1.19311802]\n",
      "SD1 is correct\n",
      "SD2 is correct\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "files = [\"16.txt\", \"25.txt\", \"50.txt\", \"64.txt\"]\n",
    "tolerance = 1e-8\n",
    "\n",
    "for file in files:\n",
    "    A_csr_tuple = load_as_csr(file)\n",
    "    n = int(file.split('.')[0])\n",
    "    b = np.random.default_rng(0).random(n)  # random RHS\n",
    "    # Solve using both versions\n",
    "    x1, iterations1 = steepest_descent_1(A_csr_tuple, b, tolerance)\n",
    "    x2, iterations2 = steepest_descent_2(A_csr_tuple, b, tolerance)\n",
    "\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Steepest Descent 1 iterations: {iterations1}\")\n",
    "    print(f\"Steepest Descent 2 iterations: {iterations2}\")\n",
    "\n",
    "    # Compare with exact solution from spsolve\n",
    "    I, J, V = A_csr_tuple\n",
    "    A_csr = csr_matrix((V, J, I), shape=(len(I)-1, max(J)+1))\n",
    "    x_exact = spsolve(A_csr, b)\n",
    "\n",
    "    # Basic correctness check\n",
    "    # (direct float comparison can be tricky, so in practice, one might compare norms)\n",
    "    print(\"\\nactual answer\", x_exact)\n",
    "    # Simple check\n",
    "    if np.allclose(x1, x_exact, rtol=1e-6, atol=1e-10):\n",
    "        print(\"SD1 is correct\")\n",
    "    if np.allclose(x2, x_exact, rtol=1e-6, atol=1e-10):\n",
    "        print(\"SD2 is correct\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7e1bb",
   "metadata": {},
   "source": [
    "**Reflections on Steepest Descent**  \n",
    "- `steepest_descent_1` performs 2 matrix-vector products per iteration (`matvec(A, x)` and `matvec(A, r)`).  \n",
    "- `steepest_descent_2` reuses the vector $z = Ar$, saving a bit of computation each iteration.  \n",
    "- In practice, `steepest_descent_2` often converges in the same number of steps but with fewer total flops.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Convergence Estimates**\n",
    "\n",
    "A well-known theoretical bound says that steepest descent requires approximately  \n",
    "$$\n",
    "\\frac{\\kappa(A) - 1}{\\kappa(A) + 1} \\quad\\text{and}\\quad\n",
    "\\text{(a related formula for CG involves } \\sqrt{\\kappa(A)}\\text{)},\n",
    "$$  \n",
    "where $\\kappa(A)$ is the condition number of $A$. We can estimate the number of iterations by rearranging that ratio into a tolerance formula.\n",
    "\n",
    "### **4.1. Condition Number and Estimates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "249319a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 16.txt, Condition Number: 126.85801089570502, Estimated Iterations: 1169.0\n",
      "File: 25.txt, Condition Number: 100.00000000000485, Estimated Iterations: 922.0\n",
      "File: 50.txt, Condition Number: 100.00000000000337, Estimated Iterations: 922.0\n",
      "File: 64.txt, Condition Number: 170.84087457719286, Estimated Iterations: 1574.0\n"
     ]
    }
   ],
   "source": [
    "def condition_number(A):\n",
    "    \"\"\"\n",
    "    Approximates the condition number of the matrix in A (CSR format)\n",
    "    using the largest and smallest eigenvalues (via eigsh).\n",
    "    \"\"\"\n",
    "    A_sp = sp.csr_matrix((A[2], A[1], A[0]))\n",
    "    lmin, _ = sp.linalg.eigsh(A_sp, k=1, which=\"SM\")\n",
    "    lmax, _ = sp.linalg.eigsh(A_sp, k=1, which=\"LM\")\n",
    "    return lmax[0]/lmin[0]\n",
    "\n",
    "def estimate_iterations(condition_num, tol):\n",
    "    r = (condition_num - 1) / (condition_num + 1)\n",
    "    return np.ceil(np.log(tol) / np.log(r))\n",
    "\n",
    "files = [\"16.txt\", \"25.txt\", \"50.txt\", \"64.txt\"]\n",
    "tol = 1e-8\n",
    "\n",
    "for file in files:\n",
    "    A_tuple = load_as_csr(file)\n",
    "    cond_num = condition_number(A_tuple)\n",
    "    its_needed = estimate_iterations(cond_num, tol)\n",
    "    print(f\"File: {file}, Condition Number: {cond_num}, Estimated Iterations: {its_needed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97277bcd",
   "metadata": {},
   "source": [
    "Observe how these estimates relate to the actual iteration counts from steepest descent above. Typically, the estimates are upper bounds or close approximations—actual iteration counts might be lower.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Conjugate Gradient (CG)**\n",
    "\n",
    "Next, we implement the **Conjugate Gradient** method for symmetric positive-definite (SPD) systems. This method converges much faster than steepest descent in many cases because it chooses directions that minimize the error in a conjugate subspace.\n",
    "\n",
    "### **5.1. Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b2fdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, tol):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient (CG) implementation from scratch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : tuple (I, J, V) in CSR format\n",
    "    b : np.ndarray\n",
    "        Right-hand side vector\n",
    "    tol : float\n",
    "        Convergence tolerance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        Approximate solution to A x = b\n",
    "    it : int\n",
    "        Number of iterations performed\n",
    "    \"\"\"\n",
    "    max_it = 2000\n",
    "    n = len(b)\n",
    "    x = np.zeros(n)\n",
    "    r = b - matvec(A, x)\n",
    "    d = r.copy()\n",
    "    \n",
    "    z_dot_z = np.dot(r, r)\n",
    "    it = 0\n",
    "\n",
    "    while True:\n",
    "        Ad = matvec(A, d)\n",
    "        alpha = z_dot_z / np.dot(d, Ad)\n",
    "        x += alpha * d\n",
    "        r -= alpha * Ad\n",
    "        r_dot_r = np.dot(r, r)\n",
    "        beta = r_dot_r / z_dot_z\n",
    "        d = r + beta * d\n",
    "        z_dot_z = r_dot_r\n",
    "        it += 1\n",
    "\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            print(f\"Converged in {it} iterations\")\n",
    "            break\n",
    "        if it >= max_it:\n",
    "            print(\"Max iterations reached\")\n",
    "            break\n",
    "    return x, it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c069b18",
   "metadata": {},
   "source": [
    "### **5.2. Testing CG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "482fcbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 17 iterations\n",
      "\n",
      "File: 16.txt\n",
      "CG iterations: 17\n",
      "\n",
      "actual answer [0.53198622 0.09899997 0.68102428 0.00172617 0.55390107 0.87347616\n",
      " 0.46273783 0.9043583  0.83372745 0.19049786 0.66853579 0.16824415\n",
      " 0.22832795 0.53371259 0.25781822 0.4518783 ]\n",
      "CG is correct\n",
      "------------------------------------------------------------\n",
      "Converged in 31 iterations\n",
      "\n",
      "File: 25.txt\n",
      "CG iterations: 31\n",
      "\n",
      "actual answer [ 5.16246348e+01 -5.72735133e+00 -1.41688466e+01  3.36685402e+00\n",
      "  1.30856991e+01  2.35415246e+00  3.95129133e+00  3.11327406e+00\n",
      " -6.63028231e+00  5.25829866e+00  5.40795979e+00 -7.92076305e-01\n",
      "  2.09922311e+01 -5.72575914e-02  2.15755518e+01 -9.94338413e-01\n",
      "  5.61752076e+00  7.90741857e+00  5.37835719e+00  1.22606610e+01\n",
      " -8.65993019e+00  4.56114180e+00  3.94504901e+01  1.36704408e+00\n",
      "  6.15144214e+01]\n",
      "CG is correct\n",
      "------------------------------------------------------------\n",
      "Converged in 59 iterations\n",
      "\n",
      "File: 50.txt\n",
      "CG iterations: 59\n",
      "\n",
      "actual answer [-2.8080864   9.23668557 28.29912267 18.52262027 42.58999449 13.10938951\n",
      "  1.06616882 10.31348592  1.15300191 39.65582596  6.4944548  -7.7851771\n",
      " 27.26371422  3.72719999  7.73662715  2.20920557 30.74917317  1.59381293\n",
      "  2.16730074  1.42987395  3.67999303  0.86518831 19.05932823  5.62091452\n",
      "  4.84560238  5.56481643 10.65794813  9.37700866  7.6151763   9.92872726\n",
      " -4.27698785  3.16497636 -4.8450979  16.03866042  9.24558656 -7.22707956\n",
      " 17.36787033 15.44359543 23.04400202 11.59657089 35.5780893   8.11006317\n",
      " 15.58855842  9.66304872 24.47834898  6.0020302  17.13477001 57.31273463\n",
      " 32.14493981 16.96234526]\n",
      "CG is correct\n",
      "------------------------------------------------------------\n",
      "Converged in 45 iterations\n",
      "\n",
      "File: 64.txt\n",
      "CG iterations: 45\n",
      "\n",
      "actual answer [1.35658774 0.87592103 1.38843097 1.56763172 1.23897498 1.32127788\n",
      " 1.53041517 1.18781373 0.93622071 1.54491411 1.93620237 1.93697138\n",
      " 1.73008258 1.12639409 0.58984019 1.18092964 1.06518523 0.31167793\n",
      " 1.81930855 1.79526255 1.15755785 0.55008498 0.71944156 1.97357627\n",
      " 1.981604   1.61365999 0.34239027 1.9857609  0.02124927 1.02929036\n",
      " 1.52817103 1.07549507 0.46044719 1.0802518  0.56274028 0.51914785\n",
      " 0.31244006 1.14291915 0.78360505 0.64333702 1.65024263 1.41724975\n",
      " 1.74614042 0.26130324 2.03112921 0.87216845 1.30887995 1.0019237\n",
      " 0.72656772 1.05408224 2.09811391 0.88644213 0.39208277 1.2950662\n",
      " 1.77943248 1.25648341 0.44248737 0.28113987 0.97774685 0.43267922\n",
      " 1.34470986 0.84392556 0.41618749 1.19311802]\n",
      "CG is correct\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tolerance = 1e-8\n",
    "files = [\"16.txt\", \"25.txt\", \"50.txt\", \"64.txt\"]\n",
    "\n",
    "for file in files:\n",
    "    A_tuple = load_as_csr(file)\n",
    "    n = int(file.split('.')[0])\n",
    "    b = np.random.default_rng(0).random(n)\n",
    "    x_cg, its_cg = conjugate_gradient(A_tuple, b, tolerance)\n",
    "    \n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"CG iterations: {its_cg}\")\n",
    "    \n",
    "    # Compare with the spsolve solution\n",
    "    I, J, V = A_tuple\n",
    "    A_csr_mat = csr_matrix((V, J, I), shape=(len(I)-1, max(J)+1))\n",
    "    x_true = spsolve(A_csr_mat, b)\n",
    "    \n",
    "    print(\"\\nactual answer\", x_true)\n",
    "    if np.allclose(x_cg, x_true, rtol=1e-6, atol=1e-10):\n",
    "        print(\"CG is correct\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861533ff",
   "metadata": {},
   "source": [
    "### **5.3. CG Convergence Estimates**\n",
    "\n",
    "Recall that CG typically satisfies  \n",
    "$$\n",
    "\\|x^{(k)} - x^*\\|_A \\le 2 \\left(\\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1}\\right)^k \\|x^{(0)} - x^*\\|_A.\n",
    "$$\n",
    "\n",
    "We can estimate the required number of iterations similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72daa8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 16.txt, Condition Number: 126.85801089570502, Estimated CG Iterations: 108.0\n",
      "File: 25.txt, Condition Number: 100.00000000000544, Estimated CG Iterations: 96.0\n",
      "File: 50.txt, Condition Number: 100.00000000000333, Estimated CG Iterations: 96.0\n",
      "File: 64.txt, Condition Number: 170.8408745771942, Estimated CG Iterations: 125.0\n"
     ]
    }
   ],
   "source": [
    "def estimate_iterations(kappa, delta):\n",
    "    \"\"\"\n",
    "    Approximates how many CG iterations are needed for\n",
    "    a given condition number kappa and tolerance delta.\n",
    "    \"\"\"\n",
    "    r = (np.sqrt(kappa) - 1) / (np.sqrt(kappa) + 1)\n",
    "    return np.ceil(np.log(delta/2) / np.log(r))\n",
    "\n",
    "files = [\"16.txt\", \"25.txt\", \"50.txt\", \"64.txt\"]\n",
    "delta = 1e-8\n",
    "\n",
    "for file in files:\n",
    "    A_tuple = load_as_csr(file)\n",
    "    kappa = condition_number(A_tuple)\n",
    "    it_est = estimate_iterations(kappa, delta)\n",
    "    print(f\"File: {file}, Condition Number: {kappa}, Estimated CG Iterations: {it_est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084e545",
   "metadata": {},
   "source": [
    "We can then compare these theoretical estimates with the actual iteration counts from the CG tests above.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Preconditioning**\n",
    "\n",
    "### **6.1. Jacobi and Symmetric Gauss-Seidel**\n",
    "\n",
    "Preconditioning modifies the system $A x = b$ to $M^{-1} A x = M^{-1} b$, where $M$ is easier to invert. Below we implement two standard preconditioners:\n",
    "\n",
    "- **Jacobi**: Uses the diagonal of $A$.  \n",
    "- **Symmetric Gauss-Seidel**: Uses the lower- and upper-triangular factors for forward/backward substitution.\n",
    "\n",
    "#### **6.1.1. Extract Diagonal & Jacobi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "663a0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diag(A):\n",
    "    \"\"\"\n",
    "    Returns the diagonal entries of A (CSR) as a 1D numpy array.\n",
    "    \"\"\"\n",
    "    I, J, V = A\n",
    "    n = num_rows(A)\n",
    "    D = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        row_start = I[i]\n",
    "        row_end = I[i+1]\n",
    "        for idx in range(row_start, row_end):\n",
    "            col_idx = J[idx]\n",
    "            if col_idx == i:\n",
    "                D[i] = V[idx]\n",
    "                break\n",
    "    return D\n",
    "\n",
    "def jacobi_preconditioner(A):\n",
    "    \"\"\"\n",
    "    Creates a function M(x) = D^{-1} x for the Jacobi preconditioner.\n",
    "    \"\"\"\n",
    "    D = get_diag(A)\n",
    "    def jacobi(x):\n",
    "        return x / D\n",
    "    return jacobi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02462ee2",
   "metadata": {},
   "source": [
    "#### **6.1.2. Symmetric Gauss-Seidel**\n",
    "\n",
    "We need forward and backward solvers for the lower and upper triangular parts of $A$. These were built from scratch previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ac2feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tril_solve(A, b):\n",
    "    \"\"\"\n",
    "    Forward solve for lower-triangular part of A in CSR.\n",
    "    \"\"\"\n",
    "    I, J, V = A\n",
    "    x = np.zeros(len(b))\n",
    "    n = num_rows(A)\n",
    "    for i in range(n):\n",
    "        x[i] = b[i]\n",
    "        row_start = I[i]\n",
    "        row_end = I[i+1]\n",
    "        diag_val = 1\n",
    "        for k in range(row_start, row_end):\n",
    "            j = J[k]\n",
    "            if j < i:\n",
    "                x[i] -= V[k] * x[j]\n",
    "            elif j == i:\n",
    "                diag_val = V[k]\n",
    "        x[i] /= diag_val\n",
    "    return x\n",
    "\n",
    "def triu_solve(A, b):\n",
    "    \"\"\"\n",
    "    Backward solve for upper-triangular part of A in CSR.\n",
    "    \"\"\"\n",
    "    I, J, V = A\n",
    "    x = np.zeros(len(b))\n",
    "    n = num_rows(A)\n",
    "    for i in reversed(range(n)):\n",
    "        x[i] = b[i]\n",
    "        row_start = I[i]\n",
    "        row_end = I[i+1]\n",
    "        diag_val = 1\n",
    "        for k in range(row_start, row_end):\n",
    "            j = J[k]\n",
    "            if j > i:\n",
    "                x[i] -= V[k] * x[j]\n",
    "            elif j == i:\n",
    "                diag_val = V[k]\n",
    "        x[i] /= diag_val\n",
    "    return x\n",
    "\n",
    "def symmetric_gauss_seidel_preconditioner(A):\n",
    "    \"\"\"\n",
    "    Creates a function M(x) corresponding to Symmetric Gauss-Seidel: y = L^{-1} D U^{-1} x.\n",
    "    \"\"\"\n",
    "    def sgs(x):\n",
    "        Uinv_x = triu_solve(A, x)\n",
    "        D = get_diag(A)\n",
    "        D_Uinv_x = D * Uinv_x\n",
    "        return tril_solve(A, D_Uinv_x)\n",
    "    return sgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53ce91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **6.2. Preconditioned Conjugate Gradient (PCG)**\n",
    "\n",
    "Finally, we implement a **Preconditioned CG** routine, closely mirroring the standard CG algorithm but applying $M$ to residual vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94d3f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preconditioned_conjugate_gradient(A, M, b, tol):\n",
    "    \"\"\"\n",
    "    Preconditioned Conjugate Gradient method (Algorithm 2 style).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : tuple\n",
    "        CSR matrix representation of A.\n",
    "    M : function\n",
    "        Preconditioner function that computes M(x).\n",
    "    b : np.ndarray\n",
    "        Right-hand side vector\n",
    "    tol : float\n",
    "        Tolerance\n",
    "    \"\"\"\n",
    "    max_it = 2000\n",
    "    it = 0\n",
    "    n = len(b)\n",
    "    x = np.zeros(n)\n",
    "\n",
    "    s = b - matvec(A, x)  # residual\n",
    "    r = M(s)             # M^-1 * s\n",
    "    d = r.copy()\n",
    "    Ad = matvec(A, d)\n",
    "\n",
    "    s_dot_r = np.dot(s, r)\n",
    "    \n",
    "    while True:\n",
    "        alpha = s_dot_r / np.dot(d, Ad)\n",
    "        x += alpha * d\n",
    "        z = s.copy()\n",
    "        s -= alpha * Ad\n",
    "        Mz = r\n",
    "        r = M(s)\n",
    "        s_dot_r_new = np.dot(s, r)\n",
    "        beta = s_dot_r_new / np.dot(z, Mz)\n",
    "        d = r + beta * d\n",
    "        Ad = matvec(A, d)\n",
    "        s_dot_r = s_dot_r_new\n",
    "\n",
    "        it += 1\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            print(f\"PCG converged in {it} iterations\")\n",
    "            break\n",
    "        if it >= max_it:\n",
    "            print(\"Max iterations reached (PCG)\")\n",
    "            break\n",
    "    return x, it + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc718f",
   "metadata": {},
   "source": [
    "Note that each iteration uses one `matvec(A, d)` call and one application of the preconditioner $M$. This is typically referred to as the “optimized” form of PCG.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Experiments with Preconditioning**\n",
    "\n",
    "We test each matrix with **Symmetric Gauss-Seidel** (SGS) vs. **Jacobi** as the preconditioner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "333301cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCG converged in 9 iterations\n",
      "PCG converged in 15 iterations\n",
      "\n",
      "File: 16.txt\n",
      "Results with Symmetric Gauss-Seidel => Iterations: 10\n",
      "Results with Jacobi => Iterations: 16\n",
      "\n",
      "actual answer [0.53198622 0.09899997 0.68102428 0.00172617 0.55390107 0.87347616\n",
      " 0.46273783 0.9043583  0.83372745 0.19049786 0.66853579 0.16824415\n",
      " 0.22832795 0.53371259 0.25781822 0.4518783 ]\n",
      "SGS is correct\n",
      "Jacobi is correct\n",
      "------------------------------------------------------------\n",
      "PCG converged in 13 iterations\n",
      "PCG converged in 24 iterations\n",
      "\n",
      "File: 25.txt\n",
      "Results with Symmetric Gauss-Seidel => Iterations: 14\n",
      "Results with Jacobi => Iterations: 25\n",
      "\n",
      "actual answer [ 5.16246348e+01 -5.72735133e+00 -1.41688466e+01  3.36685402e+00\n",
      "  1.30856991e+01  2.35415246e+00  3.95129133e+00  3.11327406e+00\n",
      " -6.63028231e+00  5.25829866e+00  5.40795979e+00 -7.92076305e-01\n",
      "  2.09922311e+01 -5.72575914e-02  2.15755518e+01 -9.94338413e-01\n",
      "  5.61752076e+00  7.90741857e+00  5.37835719e+00  1.22606610e+01\n",
      " -8.65993019e+00  4.56114180e+00  3.94504901e+01  1.36704408e+00\n",
      "  6.15144214e+01]\n",
      "SGS is correct\n",
      "Jacobi is correct\n",
      "------------------------------------------------------------\n",
      "PCG converged in 23 iterations\n",
      "PCG converged in 46 iterations\n",
      "\n",
      "File: 50.txt\n",
      "Results with Symmetric Gauss-Seidel => Iterations: 24\n",
      "Results with Jacobi => Iterations: 47\n",
      "\n",
      "actual answer [-2.8080864   9.23668557 28.29912267 18.52262027 42.58999449 13.10938951\n",
      "  1.06616882 10.31348592  1.15300191 39.65582596  6.4944548  -7.7851771\n",
      " 27.26371422  3.72719999  7.73662715  2.20920557 30.74917317  1.59381293\n",
      "  2.16730074  1.42987395  3.67999303  0.86518831 19.05932823  5.62091452\n",
      "  4.84560238  5.56481643 10.65794813  9.37700866  7.6151763   9.92872726\n",
      " -4.27698785  3.16497636 -4.8450979  16.03866042  9.24558656 -7.22707956\n",
      " 17.36787033 15.44359543 23.04400202 11.59657089 35.5780893   8.11006317\n",
      " 15.58855842  9.66304872 24.47834898  6.0020302  17.13477001 57.31273463\n",
      " 32.14493981 16.96234526]\n",
      "SGS is correct\n",
      "Jacobi is correct\n",
      "------------------------------------------------------------\n",
      "PCG converged in 16 iterations\n",
      "PCG converged in 35 iterations\n",
      "\n",
      "File: 64.txt\n",
      "Results with Symmetric Gauss-Seidel => Iterations: 17\n",
      "Results with Jacobi => Iterations: 36\n",
      "\n",
      "actual answer [1.35658774 0.87592103 1.38843097 1.56763172 1.23897498 1.32127788\n",
      " 1.53041517 1.18781373 0.93622071 1.54491411 1.93620237 1.93697138\n",
      " 1.73008258 1.12639409 0.58984019 1.18092964 1.06518523 0.31167793\n",
      " 1.81930855 1.79526255 1.15755785 0.55008498 0.71944156 1.97357627\n",
      " 1.981604   1.61365999 0.34239027 1.9857609  0.02124927 1.02929036\n",
      " 1.52817103 1.07549507 0.46044719 1.0802518  0.56274028 0.51914785\n",
      " 0.31244006 1.14291915 0.78360505 0.64333702 1.65024263 1.41724975\n",
      " 1.74614042 0.26130324 2.03112921 0.87216845 1.30887995 1.0019237\n",
      " 0.72656772 1.05408224 2.09811391 0.88644213 0.39208277 1.2950662\n",
      " 1.77943248 1.25648341 0.44248737 0.28113987 0.97774685 0.43267922\n",
      " 1.34470986 0.84392556 0.41618749 1.19311802]\n",
      "SGS is correct\n",
      "Jacobi is correct\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tolerance = 1e-8\n",
    "files = [\"16.txt\", \"25.txt\", \"50.txt\", \"64.txt\"]\n",
    "\n",
    "for file in files:\n",
    "    A_tuple = load_as_csr(file)\n",
    "    n = int(file.split('.')[0])\n",
    "    b = np.random.default_rng(0).random(n)\n",
    "\n",
    "    # Symmetric Gauss-Seidel\n",
    "    M_sgs = symmetric_gauss_seidel_preconditioner(A_tuple)\n",
    "    x_sgs, its_sgs = preconditioned_conjugate_gradient(A_tuple, M_sgs, b, tolerance)\n",
    "\n",
    "    # Jacobi\n",
    "    M_jacobi = jacobi_preconditioner(A_tuple)\n",
    "    x_jacobi, its_jacobi = preconditioned_conjugate_gradient(A_tuple, M_jacobi, b, tolerance)\n",
    "\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Results with Symmetric Gauss-Seidel => Iterations: {its_sgs}\")\n",
    "    print(f\"Results with Jacobi => Iterations: {its_jacobi}\")\n",
    "\n",
    "    # Compare with exact solution\n",
    "    I, J, V = A_tuple\n",
    "    A_csr_test = csr_matrix((V, J, I), shape=(len(I)-1, max(J)+1))\n",
    "    x_exact = spsolve(A_csr_test, b)\n",
    "    print(\"\\nactual answer\", x_exact)\n",
    "\n",
    "    # Quick correctness checks\n",
    "    if np.allclose(x_sgs, x_exact, rtol=1e-6, atol=1e-10):\n",
    "        print(\"SGS is correct\")\n",
    "    if np.allclose(x_jacobi, x_exact, rtol=1e-6, atol=1e-10):\n",
    "        print(\"Jacobi is correct\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff4c5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **8. Reflections and Trade-Offs**\n",
    "\n",
    "1. **Comparison of Methods**  \n",
    "   - **Steepest Descent**: Easiest to implement but generally slow convergence, highly sensitive to condition number.  \n",
    "   - **Conjugate Gradient**: Substantially faster for symmetric positive-definite matrices, leveraging conjugate directions.  \n",
    "   - **Preconditioned CG**: Can drastically reduce iteration counts when a good $M$ is chosen, especially for ill-conditioned problems.\n",
    "\n",
    "2. **Preconditioner Trade-Offs**  \n",
    "   - **Jacobi** is simple (just diagonal inversion) but often provides modest improvement.  \n",
    "   - **Symmetric Gauss-Seidel** typically converges faster but requires more work (forward and backward passes).  \n",
    "   - A more complex preconditioner can mean fewer iterations but higher cost per iteration.\n",
    "\n",
    "3. **Practical Considerations**  \n",
    "   - The best choice depends on matrix structure. If $A$ is close to diagonal, Jacobi might be enough. If $A$ is more dense or has more complex coupling, a stronger preconditioner (like SGS or incomplete factorization) might be preferred.  \n",
    "   - Real-world usage frequently employs specialized preconditioners tuned to the physics or geometry behind $A$.\n",
    "\n",
    "Through these implementations, we see how “from scratch” methods can reveal deeper insights into the interplay between condition numbers, iteration counts, and overall computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "# **Conclusion**\n",
    "\n",
    "In this project, we constructed a complete pipeline for tackling sparse linear systems. The process begins with loading and handling matrices in CSR format, followed by two variations of the Steepest Descent method as a baseline for iterative solves. We then explored the Conjugate Gradient method, which is especially effective for symmetric positive-definite systems, and extended it by incorporating Jacobi and Symmetric Gauss-Seidel preconditioners to form a Preconditioned Conjugate Gradient approach. We verified the accuracy of these implementations by comparing solutions against SciPy’s `spsolve` and examined their convergence patterns in light of theoretical estimates tied to the condition number. The results indicate that more sophisticated methods, particularly Conjugate Gradient and its preconditioned variant, converge significantly faster than Steepest Descent, with strong preconditioners yielding further gains in efficiency. We welcome any questions and encourage suggestions for future enhancements, such as incomplete LU or incomplete Cholesky preconditioners, as well as potential applications to real-world PDE problems.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
